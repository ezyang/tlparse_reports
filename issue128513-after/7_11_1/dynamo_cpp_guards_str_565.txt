
TREE_GUARD_MANAGER:
+- RootGuardManager
| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards
| +- GLOBAL_STATE: ___check_global_state()
| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
| | +- ID_MATCH: ___check_obj_id(L['self'], 139838528552720)                 
| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor
| | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)
| | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 7685824)                
| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)
| | | | +- GuardManager: source=L['self'].attention, accessed_by=DictGetItemGuardAccessor(attention)
| | | | | +- ID_MATCH: ___check_obj_id(L['self'].attention, 139839202022480)       
| | | | | +- GuardManager: source=L['self'].attention.__dict__, accessed_by=GetGenericDictGuardAccessor
| | | | | | +- GuardManager: source=L['self'].attention.training, accessed_by=DictGetItemGuardAccessor(training)
| | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].attention.training, 7685824)      
| +- GuardManager: source=L['to_mask'], accessed_by=DictGetItemGuardAccessor(to_mask)
| | +- TENSOR_MATCH: check_tensor(L['to_mask'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU, AutocastCPU), torch.float32, device=None, requires_grad=False, size=[1, 1, 1, 832], stride=[832, 832, 832, 1])
| | +- NO_HASATTR: hasattr(L['to_mask'], '_dynamo_dynamic_indices') == False   
| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['to_mask'], L['band_mask'], L['from_mask'], L['hidden_states'], L['blocked_encoder_mask'])
| +- GuardManager: source=L['band_mask'], accessed_by=DictGetItemGuardAccessor(band_mask)
| | +- TENSOR_MATCH: check_tensor(L['band_mask'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU, AutocastCPU), torch.float32, device=None, requires_grad=False, size=[1, 1, 9, 64, 192], stride=[110592, 110592, 12288, 192, 1])
| | +- NO_HASATTR: hasattr(L['band_mask'], '_dynamo_dynamic_indices') == False 
| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['to_mask'], L['band_mask'], L['from_mask'], L['hidden_states'], L['blocked_encoder_mask'])
| +- GuardManager: source=L['from_mask'], accessed_by=DictGetItemGuardAccessor(from_mask)
| | +- TENSOR_MATCH: check_tensor(L['from_mask'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU, AutocastCPU), torch.float32, device=None, requires_grad=False, size=[1, 1, 832, 1], stride=[832, 832, 1, 1])
| | +- NO_HASATTR: hasattr(L['from_mask'], '_dynamo_dynamic_indices') == False 
| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['to_mask'], L['band_mask'], L['from_mask'], L['hidden_states'], L['blocked_encoder_mask'])
| +- GuardManager: source=L['head_mask'], accessed_by=DictGetItemGuardAccessor(head_mask)
| | +- ID_MATCH: ___check_obj_id(L['head_mask'], 7636800)                    
| +- GuardManager: source=L['hidden_states'], accessed_by=DictGetItemGuardAccessor(hidden_states)
| | +- TENSOR_MATCH: check_tensor(L['hidden_states'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU, AutocastCPU), torch.float32, device=None, requires_grad=False, size=[1, 832, 768], stride=[638976, 768, 1])
| | +- NO_HASATTR: hasattr(L['hidden_states'], '_dynamo_dynamic_indices') == False
| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['to_mask'], L['band_mask'], L['from_mask'], L['hidden_states'], L['blocked_encoder_mask'])
| +- GuardManager: source=L['attention_mask'], accessed_by=DictGetItemGuardAccessor(attention_mask)
| | +- ID_MATCH: ___check_obj_id(L['attention_mask'], 7636800)               
| +- GuardManager: source=L['past_key_value'], accessed_by=DictGetItemGuardAccessor(past_key_value)
| | +- ID_MATCH: ___check_obj_id(L['past_key_value'], 7636800)               
| +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)
| | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 7685824)            
| +- GuardManager: source=L['blocked_encoder_mask'], accessed_by=DictGetItemGuardAccessor(blocked_encoder_mask)
| | +- TENSOR_MATCH: check_tensor(L['blocked_encoder_mask'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU, AutocastCPU), torch.float32, device=None, requires_grad=False, size=[1, 13, 64], stride=[832, 64, 1])
| | +- NO_HASATTR: hasattr(L['blocked_encoder_mask'], '_dynamo_dynamic_indices') == False
| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['to_mask'], L['band_mask'], L['from_mask'], L['hidden_states'], L['blocked_encoder_mask'])
| +- GuardManager: source=L['encoder_hidden_states'], accessed_by=DictGetItemGuardAccessor(encoder_hidden_states)
| | +- ID_MATCH: ___check_obj_id(L['encoder_hidden_states'], 7636800)        
| +- GuardManager: source=L['encoder_attention_mask'], accessed_by=DictGetItemGuardAccessor(encoder_attention_mask)
| | +- ID_MATCH: ___check_obj_id(L['encoder_attention_mask'], 7636800)       
