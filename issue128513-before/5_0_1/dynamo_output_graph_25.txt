class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_1_: "i64[1, 832][832, 1]cpu", L_stack0_2_: "f32[1, 832][832, 1]cpu", L_stack0_3_: "i64[1, 832][832, 1]cpu"):
        l_stack0_1_ = L_stack0_1_
        l_stack0_2_ = L_stack0_2_
        l_stack0_3_ = L_stack0_3_
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2200 in create_masks_for_block_sparse_attn, code: blocked_encoder_mask = attention_mask.view(batch_size, seq_length // block_size, block_size)
        blocked_encoder_mask: "f32[1, 13, 64][832, 64, 1]cpu" = l_stack0_2_.view(1, 13, 64)
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2194 in create_band_mask_from_inputs, code: [to_blocked_mask[:, 1:-3], to_blocked_mask[:, 2:-2], to_blocked_mask[:, 3:-1]], dim=2
        getitem: "f32[1, 9, 64][832, 64, 1]cpu" = blocked_encoder_mask[(slice(None, None, None), slice(1, -3, None))]
        getitem_1: "f32[1, 9, 64][832, 64, 1]cpu" = blocked_encoder_mask[(slice(None, None, None), slice(2, -2, None))]
        getitem_2: "f32[1, 9, 64][832, 64, 1]cpu" = blocked_encoder_mask[(slice(None, None, None), slice(3, -1, None))]
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2193 in create_band_mask_from_inputs, code: exp_blocked_to_pad = torch.cat(
        exp_blocked_to_pad: "f32[1, 9, 192][1728, 192, 1]cpu" = torch.cat([getitem, getitem_1, getitem_2], dim = 2);  getitem = getitem_1 = getitem_2 = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2196 in create_band_mask_from_inputs, code: band_mask = torch.einsum("blq,blk->blqk", from_blocked_mask[:, 2:-2], exp_blocked_to_pad)
        getitem_3: "f32[1, 9, 64][832, 64, 1]cpu" = blocked_encoder_mask[(slice(None, None, None), slice(2, -2, None))]
        band_mask: "f32[1, 1, 9, 64, 192][110592, 110592, 12288, 192, 1]cpu" = torch.functional.einsum('blq,blk->blqk', getitem_3, exp_blocked_to_pad);  getitem_3 = exp_blocked_to_pad = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2197 in create_band_mask_from_inputs, code: band_mask.unsqueeze_(1)
        unsqueeze_: "f32[1, 1, 9, 64, 192][110592, 110592, 12288, 192, 1]cpu" = band_mask.unsqueeze_(1)
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2203 in create_masks_for_block_sparse_attn, code: from_mask = attention_mask.view(batch_size, 1, seq_length, 1)
        from_mask: "f32[1, 1, 832, 1][832, 832, 1, 1]cpu" = l_stack0_2_.view(1, 1, 832, 1)
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:2204 in create_masks_for_block_sparse_attn, code: to_mask = attention_mask.view(batch_size, 1, 1, seq_length)
        to_mask: "f32[1, 1, 1, 832][832, 832, 832, 1]cpu" = l_stack0_2_.view(1, 1, 1, 832);  l_stack0_2_ = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:282 in forward, code: position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]
        l__self___embeddings_position_ids: "i64[1, 4096][4096, 1]cpu" = self.L__self___embeddings_position_ids
        position_ids: "i64[1, 832][4096, 1]cpu" = l__self___embeddings_position_ids[(slice(None, None, None), slice(0, 832, None))];  l__self___embeddings_position_ids = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:296 in forward, code: inputs_embeds = self.word_embeddings(input_ids)
        inputs_embeds: "f32[1, 832, 768][638976, 768, 1]cpu" = self.L__self___embeddings_word_embeddings(l_stack0_1_);  l_stack0_1_ = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:301 in forward, code: token_type_embeddings = self.token_type_embeddings(token_type_ids)
        token_type_embeddings: "f32[1, 832, 768][638976, 768, 1]cpu" = self.L__self___embeddings_token_type_embeddings(l_stack0_3_);  l_stack0_3_ = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:303 in forward, code: embeddings = inputs_embeds + token_type_embeddings
        embeddings: "f32[1, 832, 768][638976, 768, 1]cpu" = inputs_embeds + token_type_embeddings;  inputs_embeds = token_type_embeddings = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:305 in forward, code: position_embeddings = self.position_embeddings(position_ids)
        position_embeddings: "f32[1, 832, 768][638976, 768, 1]cpu" = self.L__self___embeddings_position_embeddings(position_ids);  position_ids = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:306 in forward, code: embeddings += position_embeddings
        embeddings += position_embeddings;  embeddings_1: "f32[1, 832, 768][638976, 768, 1]cpu" = embeddings;  embeddings = position_embeddings = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:308 in forward, code: embeddings = self.dropout(embeddings)
        embeddings_2: "f32[1, 832, 768][638976, 768, 1]cpu" = self.L__self___embeddings_dropout(embeddings_1);  embeddings_1 = None
        
        # File: /localdisk/leslie/miniconda/envs/pytorch_community/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py:309 in forward, code: embeddings = self.LayerNorm(embeddings)
        embeddings_3: "f32[1, 832, 768][638976, 768, 1]cpu" = self.L__self___embeddings_LayerNorm(embeddings_2);  embeddings_2 = None
        return (embeddings_3, band_mask, from_mask, to_mask, blocked_encoder_mask)
        